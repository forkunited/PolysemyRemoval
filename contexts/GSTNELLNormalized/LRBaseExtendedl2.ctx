value randomSeed="1";
value maxThreads="33";
value trainOnDev="false";
value errorExampleExtractor="FirstTokenSpan";
array validLabels=("bodypart","militaryconflict","visualartform","sport","academicfield","fruit","bakedgood","officeitem","disease","animal","wine");

evaluation accuracy=Accuracy();
evaluation accuracyBase=Accuracy(computeBaseline="true");
evaluation f.5=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="0.5");
evaluation f1=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="1");
evaluation prec=Precision(weighted="false", filterLabel="true");
evaluation recall=Recall(weighted="false", filterLabel="true");

(composite) evaluation fbPrec=LabelsListFreebase(evaluationType="Precision");
(composite) evaluation fbPrecBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Precision");
(composite) evaluation fbPrecBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Precision");
(composite) evaluation fbPrecBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Precision");
(composite) evaluation fbRecall=LabelsListFreebase(evaluationType="Recall");
(composite) evaluation fbRecallBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Recall");
(composite) evaluation fbRecallBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Recall");
(composite) evaluation fbRecallBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Recall");
(composite) evaluation fbF1=LabelsListFreebase(evaluationType="F1");
(composite) evaluation fbF1Base.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="F1");
(composite) evaluation fbF1Base.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="F1");
(composite) evaluation fbF1Base.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="F1");
(composite) evaluation fbAccuracy=LabelsListFreebase(evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Accuracy");

feature fcpna1=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((((<p:RB,VB>)*<p:VB>)|POS)(DT)?(<p:JJ,NN>)*<p:NN>).*");
feature fcpna2=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((((<p:RB,VB>)*<p:VB>)|POS){~'that'}{~'because'}(IN)+).*");
feature fcpnb1=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb2=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN,JJ,PRP> ((<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpnb3=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb4=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN> ((<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpnb5=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb6=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN,JJ> ((<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpb1=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".* (IN|TO|JJ)");
feature fcpa1=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((<p:RB,VB>)*<p:VB>).*");

feature fdep=NGramDep(scale="INDICATOR", mode="ParentsAndChildren", useRelationTypes="true", minFeatureOccurrence="2", n="1", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans");
feature fner=Ner(useTypes="true", tokenExtractor="AllTokenSpans");
feature ftcnt=TokenCount(maxCount="5", tokenExtractor="AllTokenSpans");
feature fform=StringForm(stringExtractor="FirstTokenSpan", minFeatureOccurrence="2");

feature fgnp=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="FirstTokenSpan", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnps=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="SentenceNELLNounPhrases", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd1=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentUnigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd2=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentBigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd3=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentTrigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");

ts_fn head=Head();
ts_fn ins1=NGramInside(n="1", noHead="true");
ts_fn ins2=NGramInside(n="2", noHead="true");
ts_fn ins3=NGramInside(n="3", noHead="true");
ts_fn ctxb1=NGramContext(n="1", type="BEFORE");
ts_fn ctxb2=NGramContext(n="2", type="BEFORE");
ts_fn ctxb3=NGramContext(n="3", type="BEFORE");
ts_fn ctxa1=NGramContext(n="1", type="AFTER");
ts_fn ctxb2=NGramContext(n="2", type="AFTER");
ts_fn ctxb3=NGramContext(n="3", type="AFTER");
ts_fn sent1=NGramSentence(n="1", noSpan="true");
ts_fn sent2=NGramSentence(n="2", noSpan="true");
ts_fn doc1=NGramDocument(n="1", noSentence="true");
ts_str_fn pos=PoS();
ts_str_fn strDef=String(cleanFn="PolyDefaultCleanFn");
ts_str_fn strStem=String(cleanFn="PolyStemCleanFn");
ts_str_fn strBoW=String(cleanFn="PolyBagOfWordsCleanFn");
str_fn pre3=Affix(n="3",type="PREFIX"); 
str_fn suf3=Affix(n="3",type="SUFFIX"); 
str_fn pre4=Affix(n="3",type="PREFIX"); 
str_fn suf4=Affix(n="3",type="SUFFIX"); 
str_fn pre5=Affix(n="3",type="PREFIX"); 
str_fn suf5=Affix(n="3",type="SUFFIX"); 
str_fn filter=Filter(type="SUBSTRING");
str_fn filter_s=Filter(type="SUFFIX");
str_fn filter_p=Filter(type="PREFIX");

feature fpos=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=${pos});
feature fposb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${pos} o ${ctxb1}));
feature fposb2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${pos} o ${ctxb2}));
feature fposb3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${pos} o ${ctxb3}));
feature fposa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pos} o ${ctxa1}));
feature fposa2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pos} o ${ctxa2}));
feature fposa3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pos} o ${ctxa3}));

feature fctxb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${strStem} o ${ctxb1}));
feature fctxb2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${strStem} o ${ctxb2}));
feature fctxb3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${strStem} o ${ctxb3}));
feature fctxa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${strStem} o ${ctxa1}));
feature fctxa2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${strStem} o ${ctxa2}));
feature fctxa3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${strStem} o ${ctxa3}));

feature fpre3h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre3} o ${strDef} o ${head}));
feature fsuf3h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf3} o ${strDef} o ${head}));
feature fpre3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre3} o ${strDef} o ${ins1}));
feature fsuf3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf3} o ${strDef} o ${ins1}));
feature fpre4h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre4} o ${strDef} o ${head}));
feature fsuf4h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf4} o ${strDef} o ${head}));
feature fpre4=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre4} o ${strDef} o ${ins1}));
feature fsuf4=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf4} o ${strDef} o ${ins1}));
feature fpre5h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre5} o ${strDef} o ${head}));
feature fsuf5h=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf5} o ${strDef} o ${head}));
feature fpre5=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_p} o ${pre5} o ${strDef} o ${ins1}));
feature fsuf5=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter_s} o ${suf5} o ${strDef} o ${ins1}));

feature fphrh1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${head}));
feature fphr1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins1}));
feature fphr2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins2}));
feature fphr3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins3}));

feature fdoc1=TokenSpanFnDataVocab(scale="NORMALIZED_TFIDF", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter} o ${strBoW} o ${doc1}));
feature fsent1=TokenSpanFnDataVocab(scale="NORMALIZED_TFIDF", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter} o ${strBoW} o ${sent1}));
feature fsent2=TokenSpanFnDataVocab(scale="NORMALIZED_TFIDF", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${filter} o ${strBoW} o ${sent2}));

model lr=Areg(l1="0", l2="0", convergenceEpsilon=".001", maxTrainingExamples="260001", batchSize="100", evaluationIterations="200", maxEvaluationConstantIterations="500", weightedLabels="false", computeTestEvaluations="false")
{
	array validLabels=${validLabels};
};

gs g=GridSearch() {
	dimension l2=Dimension(name="l2", values=(.000001,.00001,.0001), trainingDimension="true");
	dimension ct=Dimension(name="classificationThreshold", values=(.5,.6,.7,.8,.9), trainingDimension="false");
	
	model model=${lr};
	evaluation evaluation=${accuracy};
};
