value randomSeed="1";
value maxThreads="33";
value trainOnDev="false";
value errorExampleExtractor="FirstTokenSpan";
array validLabels=("bodypart","militaryconflict","visualartform","sport","academicfield","fruit","bakedgood","officeitem","disease","animal","wine");

evaluation accuracy=Accuracy();
evaluation accuracyBase=Accuracy(computeBaseline="true");
evaluation f.5=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="0.5");
evaluation f1=F(mode="MACRO_WEIGHTED", filterLabel="true", Beta="1");
evaluation prec=Precision(weighted="false", filterLabel="true");
evaluation recall=Recall(weighted="false", filterLabel="true");

(composite) evaluation fbPrec=LabelsListFreebase(evaluationType="Precision");
(composite) evaluation fbPrecBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Precision");
(composite) evaluation fbPrecBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Precision");
(composite) evaluation fbPrecBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Precision");
(composite) evaluation fbRecall=LabelsListFreebase(evaluationType="Recall");
(composite) evaluation fbRecallBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Recall");
(composite) evaluation fbRecallBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Recall");
(composite) evaluation fbRecallBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Recall");
(composite) evaluation fbF1=LabelsListFreebase(evaluationType="F1");
(composite) evaluation fbF1Base.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="F1");
(composite) evaluation fbF1Base.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="F1");
(composite) evaluation fbF1Base.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="F1");
(composite) evaluation fbAccuracy=LabelsListFreebase(evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.5=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.5", evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.75=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.75", evaluationType="Accuracy");
(composite) evaluation fbAccuracyBase.9=LabelsListFreebase(computeNELLBaseline="true", NELLConfidenceThreshold="0.9", evaluationType="Accuracy");

feature fcpna1=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((((<p:RB,VB>)*<p:VB>)|POS)(DT)?(<p:JJ,NN>)*<p:NN>).*");
feature fcpna2=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((((<p:RB,VB>)*<p:VB>)|POS){~'that'}{~'because'}(IN)+).*");
feature fcpnb1=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb2=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN,JJ,PRP> ((<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpnb3=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb4=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN> ((<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpnb5=GramContextPattern(capturePart="BEFORE", captureGroup="0", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern="^(<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*");
feature fcpnb6=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".*<<~p:CD,NNP,NN,JJ> ((<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)");
feature fcpb1=GramContextPattern(capturePart="BEFORE", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", beforePattern=".* (IN|TO|JJ)");
feature fcpa1=GramContextPattern(capturePart="AFTER", captureGroup="1", minFeatureOccurrence="2", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans", afterPattern="((<p:RB,VB>)*<p:VB>).*");

feature fdep=NGramDep(scale="INDICATOR", mode="ParentsAndChildren", useRelationTypes="true", minFeatureOccurrence="2", n="1", cleanFn="PolyStemCleanFn", tokenExtractor="AllTokenSpans");
feature fner=Ner(useTypes="true", tokenExtractor="AllTokenSpans");
feature ftcnt=TokenCount(maxCount="5", tokenExtractor="AllTokenSpans");
feature fform=StringForm(stringExtractor="FirstTokenSpan", minFeatureOccurrence="2");

feature fgnp=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="FirstTokenSpan", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnps=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="SentenceNELLNounPhrases", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd1=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentUnigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd2=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentBigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");
feature fgnpd3=GazetteerContains(gazetteer="NounPhraseNELLCategory", stringExtractor="AllDocumentTrigramsNP", includeIds="true", includeWeights="true", weightThreshold="0.90");

ts_fn head=Head();
ts_fn ins1=NGramInside(n="1", noHead="true");
ts_fn ins2=NGramInside(n="2", noHead="true");
ts_fn ins3=NGramInside(n="3", noHead="true");
ts_fn ctxb1=NGramContext(n="1", type="BEFORE");
ts_fn ctxa1=NGramContext(n="1", type="AFTER");
ts_fn ctxb2=NGramContext(n="2", type="BEFORE");
ts_fn ctxa2=NGramContext(n="2", type="AFTER");
ts_fn ctxb3=NGramContext(n="3", type="BEFORE");
ts_fn ctxa3=NGramContext(n="3", type="AFTER");
ts_fn sent1=NGramSentence(n="1", noSpan="true");
ts_fn sent2=NGramSentence(n="2", noSpan="true");
ts_fn sent3=NGramSentence(n="3", noSpan="true");
ts_fn doc1=NGramDocument(n="1", noSentence="true");
ts_fn doc2=NGramDocument(n="2", noSentence="true");
ts_str_fn pos=PoS();
ts_str_fn strDef=String(cleanFn="PolyDefaultCleanFn");
ts_str_fn strStem=String(cleanFn="PolyStemCleanFn");
ts_str_fn strBoW=String(cleanFn="PolyBagOfWordsFeatureCleanFn");
str_fn pre3=Affix(n="3",type="PREFIX"); 
str_fn suf3=Affix(n="3",type="SUFFIX"); 
str_fn pre4=Affix(n="4",type="PREFIX"); 
str_fn suf4=Affix(n="4",type="SUFFIX"); 
str_fn pre5=Affix(n="5",type="PREFIX"); 
str_fn suf5=Affix(n="5",type="SUFFIX"); 
str_fn filter=Filter(type="SUBSTRING");
str_fn filter_s=Filter(type="SUFFIX");
str_fn filter_p=Filter(type="PREFIX");

feature fpos=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=${pos});
feature fphrh1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${head}));
feature fphr1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins1}));
feature fphr2=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins2}));
feature fphr3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strDef} o ${ins3}));

feature fposb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxb1}));
feature fposa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxa1}));
feature fctxb1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxb1}));
feature fctxa1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxa1}));
feature fpreh3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre3} o ${strDef} o ${head}));
feature fsufh3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf3} o ${strDef} o ${head}));
feature fpre3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre3} o ${strDef} o ${ins1}));
feature fsuf3=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf3} o ${strDef} o ${ins1}));
feature fsent1=TokenSpanFnDataVocab(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strBoW} o ${sent1}));

(ignored) feature fsent2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strBoW} o ${sent2}));
(ignored) feature fsent3=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strBoW} o ${sent3}));
(ignored) feature fdoc1=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strBoW} o ${doc1}));
(ignored) feature fdoc2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strBoW} o ${doc2}));
(ignored) feature fctxb2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxb2}));
(ignored) feature fctxb3=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxb3}));
(ignored) feature fctxa2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxa2}));
(ignored) feature fctxa3=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${strStem} o ${ctxa3}));
(ignored) feature fposb2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxb2}));
(ignored) feature fposb3=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxb3}));
(ignored) feature fposa2=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxa2}));
(ignored) feature fposa3=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pos} o ${ctxa3}));
(ignored) feature fpre4=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre4} o ${strDef} o ${ins1}));
(ignored) feature fpre5=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre5} o ${strDef} o ${ins1}));
(ignored) feature fsuf4=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf4} o ${strDef} o ${ins1}));
(ignored) feature fsuf5=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf5} o ${strDef} o ${ins1}));
(ignored) feature fpreh4=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre4} o ${strDef} o ${head}));
(ignored) feature fpreh5=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${pre5} o ${strDef} o ${head}));
(ignored) feature fsufh4=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf4} o ${strDef} o ${head}));
(ignored) feature fsufh5=TokenSpanFnDataVocabTrie(scale="INDICATOR", minFeatureOccurrence="2", tokenExtractor="AllTokenSpans", fn=(${suf5} o ${strDef} o ${head}));

rs rules=RuleSet() {
	rule sentDoc1 = (TokenSpanFnDataVocab(fn=(${strBoW} o ${sent1})))
						-> (TokenSpanFnFilteredVocab(
							vocabFeature=${fdoc1}, vocabFilter=${FEATURE_STR}, vocabFilterType="EQUAL", fn=(${strBoW} o ${doc1}), tokenExtractor="AllTokenSpans"));
	
};

model lg=LogistmarGramression(rules=${rules}, t=".2", l2="0", convergenceEpsilon=".001", maxTrainingExamples="260001", batchSize="100", evaluationIterations="200", maxEvaluationConstantIterations="500", computeTestEvaluations="false")
{
	array validLabels=${validLabels};
};

gs g=GridSearch() {
	dimension l2=Dimension(name="l2", values=(.000001,.00001,.0001), trainingDimension="true");
	dimension ct=Dimension(name="classificationThreshold", values=(.5,.6,.7,.8,.9), trainingDimension="false");

	model model=${lg};
	evaluation evaluation=${accuracy};
};
