randomSeed=1
maxThreads=33
trainOnDev=false
errorExampleExtractor=FirstTokenSpan

iterations=3
firstIterationOnlyLabeled=true

evaluation=Accuracy()
evaluation=Accuracy(computeBaseline=true)
evaluation=F(mode=MACRO_WEIGHTED, filterLabel=true, Beta=0.5)
evaluation=F(mode=MACRO_WEIGHTED, filterLabel=true, Beta=1)
evaluation=Precision(weighted=false, filterLabel=true)
evaluation=Recall(weighted=false, filterLabel=true)

unlabeledEvaluation=LabelsListFreebase(evaluationType=Precision)
unlabeledEvaluation=LabelsListFreebase(computeNELLBaseline=true, NELLConfidenceThreshold=0.9, evaluationType=Precision)
unlabeledEvaluation=LabelsListFreebase(evaluationType=Recall)
unlabeledEvaluation=LabelsListFreebase(computeNELLBaseline=true, NELLConfidenceThreshold=0.9, evaluationType=Recall)
unlabeledEvaluation=LabelsListFreebase(evaluationType=Accuracy)
unlabeledEvaluation=LabelsListFreebase(computeNELLBaseline=true, NELLConfidenceThreshold=0.9, evaluationType=Accuracy)
unlabeledEvaluation=LabelsListFreebase(evaluationType=F1)
unlabeledEvaluation=LabelsListFreebase(computeNELLBaseline=true, NELLConfidenceThreshold=0.9, evaluationType=F1)
unlabeledEvaluation=Polysemy()
unlabeledEvaluation=Polysemy(computeNELLBaseline=true, NELLConfidenceThreshold=0.9)

gridDimension=l1(values="0,.0000001,.000001,.00001", training=true)
gridDimension=classificationThreshold(values=".1,.2,.3,.4,.5,.6,.7,.8,.9", training=false)

feature_cpna1=GramContextPattern(capturePart=AFTER, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, afterPattern="((((<p:RB,VB>)*<p:VB>)|POS)(DT)?(<p:JJ,NN>)*<p:NN>).*")
feature_cpna2=GramContextPattern(capturePart=AFTER, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, afterPattern="((((<p:RB,VB>)*<p:VB>)|POS){~'that'}{~'because'}(IN)+).*")
feature_cpnb1=GramContextPattern(capturePart=BEFORE, captureGroup=0, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern="^(<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*")
feature_cpnb2=GramContextPattern(capturePart=BEFORE, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern=".*<<~p:CD,NNP,NN,JJ,PRP> ((<p:NNP,NN,JJ,PRP>)+(SYM)?(<p:VB,RB>)*<p:VB>(<p:VB,RB>)*{~'that'}{~'because'}(IN|TO|JJ)*)")
feature_cpnb3=GramContextPattern(capturePart=BEFORE, captureGroup=0, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern="^(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*")
feature_cpnb4=GramContextPattern(capturePart=BEFORE, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern=".*<<~p:CD,NNP,NN> ((<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)")
feature_cpnb5=GramContextPattern(capturePart=BEFORE, captureGroup=0, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern="^(<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*")
feature_cpnb6=GramContextPattern(capturePart=BEFORE, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern=".*<<~p:CD,NNP,NN,JJ> ((<p:NN,JJ,NNP>)+(<p:NN,NNP>)(SYM)?{~'that'}{~'because'}(IN|TO|JJ)*)")
feature_cpb1=GramContextPattern(capturePart=BEFORE, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, beforePattern=".* (IN|TO|JJ)")
feature_cpa1=GramContextPattern(capturePart=AFTER, captureGroup=1, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans, afterPattern="((<p:RB,VB>)*<p:VB>).*")

feature_dep=NGramDep(scale=INDICATOR, mode=ParentsAndChildren, useRelationTypes=true, minFeatureOccurrence=2, n=1, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ner=Ner(useTypes=true, tokenExtractor=AllTokenSpans)
feature_tcnt=TokenCount(maxCount=5, tokenExtractor=AllTokenSpans)
feature_form=StringForm(stringExtractor=FirstTokenSpan, minFeatureOccurrence=2)

feature_pos=GramCluster{scale=INDICATOR, minFeatureOccurrence=2, tokenExtractor=AllTokenSpans, clusterer=PoSTag)
feature_posb1=NGramContext(scale=INDICATOR, maxGramDistance=1, n=1, mode=BEFORE, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)
feature_posb2=NGramContext(scale=INDICATOR, maxGramDistance=2, n=2, mode=BEFORE, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)
feature_posb3=NGramContext(scale=INDICATOR, maxGramDistance=3, n=3, mode=BEFORE, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)
feature_posa1=NGramContext(scale=INDICATOR, maxGramDistance=1, n=1, mode=AFTER, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)
feature_posa2=NGramContext(scale=INDICATOR, maxGramDistance=2, n=2, mode=AFTER, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)
feature_posa3=NGramContext(scale=INDICATOR, maxGramDistance=3, n=3, mode=AFTER, minFeatureOccurrence=2, clusterer=PoSTag, tokenExtractor=AllTokenSpans)

feature_ctxb1=NGramContext(scale=INDICATOR, maxGramDistance=1, n=1, mode=BEFORE, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ctxb2=NGramContext(scale=INDICATOR, maxGramDistance=2, n=2, mode=BEFORE, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ctxb3=NGramContext(scale=INDICATOR, maxGramDistance=3, n=3, mode=BEFORE, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ctxa1=NGramContext(scale=INDICATOR, maxGramDistance=1, n=1, mode=AFTER, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ctxa2=NGramContext(scale=INDICATOR, maxGramDistance=2, n=2, mode=AFTER, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)
feature_ctxa3=NGramContext(scale=INDICATOR, maxGramDistance=3, n=3, mode=AFTER, minFeatureOccurrence=2, cleanFn=PolyStemCleanFn, tokenExtractor=AllTokenSpans)

feature_phr1=NGramContext(scale=INDICATOR, mode=WITHIN, minFeatureOccurrence=2, n=1, cleanFn=PolyDefaultCleanFn, tokenExtractor=FirstTokenSpanNotLastToken)
feature_aff5=NGramContext(clusterer=AffixMaxLength5, scale=INDICATOR, mode=WITHIN, minFeatureOccurrence=2, n=1, cleanFn=PolyDefaultCleanFn, tokenExtractor=FirstTokenSpanNotLastToken)
feature_phrh1=NGramContext(scale=INDICATOR, mode=WITHIN, minFeatureOccurrence=2, n=1, cleanFn=PolyDefaultCleanFn, tokenExtractor=FirstTokenSpanLastToken)
feature_affh5=NGramContext(clusterer=AffixMaxLength5, scale=INDICATOR, mode=WITHIN, minFeatureOccurrence=2, n=1, cleanFn=PolyDefaultCleanFn, tokenExtractor=FirstTokenSpanLastToken)

feature_doc1=NGramSentence(scale=NORMALIZED_TFIDF, minFeatureOccurrence=2, n=1, cleanFn=PolyBagOfWordsFeatureCleanFn, tokenExtractor=AllDocumentSentenceInitialTokens)
feature_doc2=NGramSentence(scale=NORMALIZED_TFIDF, minFeatureOccurrence=2, n=1, cleanFn=PolyBagOfWordsFeatureCleanFn, tokenExtractor=AllDocumentSentenceInitialTokens)
feature_sent1=NGramSentence(noTokenSpan=true, scale=NORMALIZED_TFIDF, minFeatureOccurrence=2, n=1, cleanFn=PolyBagOfWordsFeatureCleanFn, tokenExtractor=AllTokenSpans)
feature_sent2=NGramSentence(noTokenSpan=true, scale=NORMALIZED_TFIDF, minFeatureOccurrence=2, n=2, cleanFn=PolyBagOfWordsFeatureCleanFn, tokenExtractor=AllTokenSpans)

feature_gnp=GazetteerContains(gazetteer=NounPhraseNELLCategory, stringExtractor=FirstTokenSpan, includeIds=true, includeWeights=true, weightThreshold=0.90)
feature_gnps=GazetteerContains(gazetteer=NounPhraseNELLCategory, stringExtractor=SentenceNELLNounPhrases, includeIds=true, includeWeights=true, weightThreshold=0.90)
feature_gnpd=GazetteerContains(gazetteer=NounPhraseNELLCategory, stringExtractor=DocumentNELLNounPhrases, includeIds=true, includeWeights=true, weightThreshold=0.90)

model=Areg(l1=0, l2=0, convergenceEpsilon=-1, maxTrainingExamples=100000, batchSize=500, evaluationIterations=500, maxEvaluationConstantIterations=5000, weightedLabels=false)
{
	validLabels=None
}
